{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXS5apwxz5i-",
        "outputId": "2bfc26e8-f352-4275-cb5a-5a581a7901e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/383.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.5/383.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/318.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet langchain\n",
        "!pip install -qU langchain-openai\n",
        "!pip install -qU pydantic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXmB5a5f0BB8",
        "outputId": "19543179-df3d-4268-cfd4-7413eeb9bbc2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tool calling\n",
        "\n",
        "We use the term \"tool calling\" interchangeably with \"function calling\". Although function calling is sometimes meant to refer to invocations of a single function, we treat all models as though they can return multiple tool or function calls in each message.\n",
        "\n",
        "\n",
        "Request: Passing tools to model\n",
        "\n",
        "For a model to be able to invoke tools, you need to pass tool schemas to it when making a chat request. LangChain ChatModels supporting tool calling features implement a .bind_tools method, which receives a list of LangChain tool objects, Pydantic classes, or JSON Schemas and binds them to the chat model in the provider-specific expected format. Subsequent invocations of the bound chat model will include tool schemas in every call to the model API.\n",
        "Defining tool schemas: LangChain Tool\n",
        "For example, we can define the schema for custom tools using the @tool decorator on Python functions:"
      ],
      "metadata": {
        "id": "YFDwSAq42H09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Define simple mathematical tools: addition and multiplication\n",
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplies a and b.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# Store tools in a list\n",
        "tools = [add, multiply]"
      ],
      "metadata": {
        "id": "fGyOgVV40_lc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the OpenAI model (GPT-3.5-turbo in this case)\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Bind tools to the model\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "ltzKEL791LCH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your query to trigger tool usage\n",
        "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
        "\n",
        "# Invoke the model with the query\n",
        "ai_message = llm_with_tools.invoke(query)\n",
        "\n",
        "# Output the tool calls made by the model\n",
        "print(\"Tool calls made by the model:\", ai_message.tool_calls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tst1-aiN1OtP",
        "outputId": "81e03aa0-1fca-48d0-ea21-9c30aef630c3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool calls made by the model: [{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_jzMBfdhav67wXBUDA92tZvb9', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_rNTbb5YcvCB2G8yTSVfgSWw8', 'type': 'tool_call'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, ToolMessage\n",
        "\n",
        "# Initialize conversation with the human query\n",
        "messages = [HumanMessage(query)]\n",
        "messages.append(ai_message)\n",
        "\n",
        "# Call the tools and pass their output back to the model\n",
        "for tool_call in ai_message.tool_calls:\n",
        "    # Determine which tool to use\n",
        "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
        "\n",
        "    # Call the tool with the provided arguments\n",
        "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
        "\n",
        "    # Append the tool output to the message list\n",
        "    messages.append(ToolMessage(str(tool_output), tool_call_id=tool_call[\"id\"]))\n",
        "\n",
        "# Display the tool outputs appended to the conversation\n",
        "for message in messages:\n",
        "    print(message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc9ozkcP1Yrd",
        "outputId": "80201d89-21ce-49ec-a819-a592e82ed36f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='What is 3 * 12? Also, what is 11 + 49?' additional_kwargs={} response_metadata={}\n",
            "content='' additional_kwargs={'tool_calls': [{'id': 'call_jzMBfdhav67wXBUDA92tZvb9', 'function': {'arguments': '{\"a\": 3, \"b\": 12}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_rNTbb5YcvCB2G8yTSVfgSWw8', 'function': {'arguments': '{\"a\": 11, \"b\": 49}', 'name': 'add'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 88, 'total_tokens': 137, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} id='run-6796e3b5-b003-4d33-9c65-18199c8cb18e-0' tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_jzMBfdhav67wXBUDA92tZvb9', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_rNTbb5YcvCB2G8yTSVfgSWw8', 'type': 'tool_call'}] usage_metadata={'input_tokens': 88, 'output_tokens': 49, 'total_tokens': 137}\n",
            "content='36' tool_call_id='call_jzMBfdhav67wXBUDA92tZvb9'\n",
            "content='60' tool_call_id='call_rNTbb5YcvCB2G8yTSVfgSWw8'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the final response from the model with the tool results included\n",
        "final_response = llm_with_tools.invoke(messages)\n",
        "print(\"Final response from the model:\", final_response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDcLUDyI1s0z",
        "outputId": "de97a15f-28e5-4544-f380-f56de99dbc73"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final response from the model: 3 * 12 is 36 and 11 + 49 is 60.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "always_call_tool_llm = llm.bind_tools([add, multiply], tool_choice=\"any\")\n"
      ],
      "metadata": {
        "id": "enpbE5RO13LE"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}